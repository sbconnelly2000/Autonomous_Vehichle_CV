Defend The Republic: Autonomous Aerial Computer Vision SystemðŸ“– Project OverviewThis repository contains the computer vision subsystem for the "Defend The Republic" (DTR) autonomous aerial competition team at Indiana University Bloomington.As a Junior Intelligent Systems Engineering student, my role focused on developing a low-latency object detection and range-finding pipeline. The system enables an autonomous blimp/drone to identify game pieces (balloons and goals), calculate their precise 3D position relative to the vehicle, and feed navigation data to the flight controller in real-time.ðŸŽ¯ Key CapabilitiesObject Detection: Identifies 8 distinct classes (Orange/Yellow Circles, Squares, Triangles; Purple/Green Balls) using a custom-trained YOLOv8 model.Edge Optimization: Optimized for Raspberry Pi 4, achieving 180ms inference time (up from 1s) via quantization and input scaling.3D Localization: Algorithms to calculate distance (meters) and angle (theta x, theta y) from 2D bounding boxes.ðŸ› ï¸ Tech StackHardware: Raspberry Pi 4 Model B, Pi Camera Module.ML Frameworks: PyTorch, Ultralytics YOLOv8.Data Processing: Roboflow (Dataset management, augmentation), OpenCV.Techniques: INT8 Quantization, Transfer Learning, Trigonometric Range Finding.âš™ï¸ System ArchitectureThe pipeline processes video input to output navigation vectors for the flight controller.graph TD
    A[Pi Camera Input] -->|320x320 Stream| B(YOLOv8 Int8 Model)
    B -->|Bounding Boxes| C{Confidence Check > 0.5}
    C -->|Yes| D[Object ID Extraction]
    C -->|No| A
    D --> E[Distance Calculation Algo]
    D --> F[Angle Calculation Algo]
    E & F --> G[Output Vector]
    G -->|UDP/Serial| H[Flight Controller]
The Data PipelineDataset: 2,000 images collected and labeled manually.Split: 70% Train, 20% Validation, 10% Test.Augmentation: Zoom, Rotation, and Dynamic Lighting adjustments to simulate field conditions.ðŸš€ Engineering Challenges & OptimizationsOne of the significant challenges we faced was the hardware limitation of the Raspberry Pi 4.1. Latency ReductionInitial State: The standard YOLOv8 model ($640\times640$ float16) resulted in a 1-second inference time, which was too slow for autonomous flight control.Solution: We employed a two-step optimization strategy:Input Scaling: Reduced input resolution to $320\times320$.Quantization: Converted model weights from Float16 to Int8.Result: Inference time dropped to 180ms (~5.5 FPS) while maintaining a high confidence threshold ($>0.73$) on game objects.2. Distance AccuracyWe implemented a custom script to calculate focal length based on real-world measurements rather than theoretical camera specs. This reduced distance estimation error significantly, allowing the drone to accurately gauge approach velocity.ðŸ“Š Performance VisualsModel VersionResolutionPrecisionInference Time (Pi 4)v1 (Baseline)640x640High~1000msv2 (Optimized)480x480High~415msv3 (Deployed)320x320 (Int8)Medium-High~180msðŸ’» Installation & UsagePrerequisitesRaspberry Pi 4 (Recommended) or Linux environmentPython 3.8+SetupClone the repository:git clone [https://github.com/](https://github.com/)[YOUR_USERNAME]/[REPO_NAME].git
cd [REPO_NAME]
Install dependencies:pip install ultralytics opencv-python numpy
Running the Inference ScriptTo start the detection loop with the optimized model:python NEW_usb_cam.py --source 0 --conf 0.5
Output Format: [object_class, confidence, theta_x, theta_y, distance]ðŸ‘¥ Team & AcknowledgementsDefend The Republic - Computer Vision TeamSamson C. (Lead / Optimization)Thanakin C.Leah H.Sam H.Yoav K.Jiaqi W.Special thanks to Professor Or and our mentors at Indiana University Bloomington for their guidance on hardware integration.ðŸ“¬ Contact[Your Name] Junior, Intelligent Systems Engineering @ IU Bloomington [LinkedIn Profile Link] | [Email Address]Looking for Summer 2026 Internship opportunities in Embedded Systems, Computer Vision, or Robotics.
